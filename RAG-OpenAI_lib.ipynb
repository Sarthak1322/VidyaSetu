{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d75d0d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5f8f4bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PROVIDER = \"openai\"\n",
    "MODEL_NAME = \"gpt-4o-mini\"\n",
    "APP_VERSION = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e0100f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üöÄ STARTING PDF BOOK WORKFLOW with UNSTRUCTURED\n",
      "==================================================\n",
      "‚úÖ Processing book directory: 'kehs1dd'\n",
      "üõë Warning: No PDF files found to process in /home/manoj/Project/RAG/kehs1dd\n",
      "\n",
      "Skipping vector store creation because no documents were extracted.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from openai import OpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS, Chroma\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "\n",
    "def get_documents_from_source(source_path):\n",
    "    \"\"\"\n",
    "    Extracts text and metadata from a source path using the 'unstructured'\n",
    "    library. It can handle a single PDF file or a directory of PDFs.\n",
    "\n",
    "    Args:\n",
    "        source_path (str): The path to the PDF file or directory.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of LangChain Document objects with rich metadata.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(source_path):\n",
    "        print(f\"üõë Error: Source path not found at {source_path}\")\n",
    "        return []\n",
    "\n",
    "    all_docs = []\n",
    "    \n",
    "    # Determine which PDF files to process\n",
    "    pdf_files_to_process = []\n",
    "    book_title = \"Single Document\"\n",
    "\n",
    "    if os.path.isdir(source_path):\n",
    "        book_title = os.path.basename(source_path)\n",
    "        print(f\"‚úÖ Processing book directory: '{book_title}'\")\n",
    "        pdf_files_to_process = [os.path.join(source_path, f) for f in sorted(os.listdir(source_path)) if f.lower().endswith('.pdf')]\n",
    "    elif os.path.isfile(source_path) and source_path.lower().endswith('.pdf'):\n",
    "        print(f\"‚úÖ Processing single PDF file: {os.path.basename(source_path)}\")\n",
    "        pdf_files_to_process.append(source_path)\n",
    "    else:\n",
    "        print(f\"üõë Error: Path '{source_path}' is not a valid PDF file or directory.\")\n",
    "        return []\n",
    "\n",
    "    if not pdf_files_to_process:\n",
    "        print(f\"üõë Warning: No PDF files found to process in {source_path}\")\n",
    "        return []\n",
    "\n",
    "    # Process each PDF file with 'unstructured'\n",
    "    for pdf_path in pdf_files_to_process:\n",
    "        chapter_title = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "        print(f\"  üìñ Processing Chapter: '{chapter_title}' with unstructured...\")\n",
    "        \n",
    "        try:\n",
    "            # The core of the new logic: partition_pdf\n",
    "            elements = partition_pdf(\n",
    "                filename=pdf_path,\n",
    "                # 'fast' is a good balance of speed and accuracy.\n",
    "                # Use 'hi_res' for more complex documents, which may require tesseract.\n",
    "                strategy=\"fast\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"üõë Error processing file {pdf_path} with unstructured: {e}\")\n",
    "            continue\n",
    "\n",
    "        current_heading = \"Introduction\" # Default for text before the first header\n",
    "        for el in elements:\n",
    "            # unstructured identifies titles and headers, which we use for chapter context\n",
    "            if el.category in (\"Title\", \"Header\", \"SubTitle\"):\n",
    "                current_heading = el.text.strip()\n",
    "            \n",
    "            # Create a Document for each element with detailed metadata\n",
    "            doc = Document(\n",
    "                page_content=el.text,\n",
    "                metadata={\n",
    "                    \"book_title\": book_title,\n",
    "                    \"chapter_file\": os.path.basename(pdf_path),\n",
    "                    \"heading_context\": current_heading,\n",
    "                    \"element_type\": el.category\n",
    "                }\n",
    "            )\n",
    "            all_docs.append(doc)\n",
    "\n",
    "    if not all_docs:\n",
    "        print(\"üõë Warning: No text could be extracted to create documents.\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Source processing complete. Total documents created: {len(all_docs)}\")\n",
    "    \n",
    "    return all_docs\n",
    "\n",
    "\n",
    "# --- FAISS Vector Store Functions ---\n",
    "\n",
    "def create_or_load_faiss_store(documents, embeddings, index_path=\"faiss_index\"):\n",
    "    if os.path.exists(index_path):\n",
    "        print(f\"‚úÖ Loading existing FAISS index from: {index_path}\")\n",
    "        vector_store = FAISS.load_local(index_path, embeddings, allow_dangerous_deserialization=True)\n",
    "    else:\n",
    "        print(f\"‚ÑπÔ∏è No FAISS index found. Creating a new one at: {index_path}\")\n",
    "        if not documents:\n",
    "            print(\"üõë Error: No documents provided to create a new FAISS store.\")\n",
    "            return None\n",
    "        vector_store = FAISS.from_documents(documents, embeddings)\n",
    "        vector_store.save_local(index_path)\n",
    "        print(f\"‚úÖ New FAISS index created and saved.\")\n",
    "    return vector_store\n",
    "\n",
    "# --- ChromaDB Vector Store Functions ---\n",
    "def create_or_load_chroma_store(documents, embeddings, persist_directory=\"chroma_db\"):\n",
    "    if os.path.exists(persist_directory):\n",
    "        print(f\"‚úÖ Loading existing ChromaDB from: {persist_directory}\")\n",
    "        vector_store = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
    "    else:\n",
    "        print(f\"‚ÑπÔ∏è No ChromaDB found. Creating a new one at: {persist_directory}\")\n",
    "        if not documents:\n",
    "            print(\"üõë Error: No documents provided to create a new ChromaDB store.\")\n",
    "            return None\n",
    "        vector_store = Chroma.from_documents(documents=documents, embedding=embeddings, persist_directory=persist_directory)\n",
    "        print(f\"‚úÖ New ChromaDB created and persisted.\")\n",
    "    return vector_store\n",
    "\n",
    "\n",
    "# --- Retrieval Function ---\n",
    "\n",
    "def perform_similarity_search(vector_store, query):\n",
    "    if not vector_store:\n",
    "        print(\"üõë Cannot perform search: Vector store is not available.\")\n",
    "        return []\n",
    "\n",
    "    print(f\"\\nüîç Performing similarity search for query: '{query}'\")\n",
    "    results = vector_store.similarity_search(query, k=3)\n",
    "    if not results:\n",
    "        print(\"   -> No results found.\")\n",
    "        return []\n",
    "        \n",
    "    for i, doc in enumerate(results):\n",
    "        content_snippet = \" \".join(doc.page_content.split())[:250] + \"...\"\n",
    "        print(f\"   üìÑ Result {i+1}: \\\"{content_snippet}\\\"\")\n",
    "        if doc.metadata:\n",
    "            book = doc.metadata.get('book_title', 'N/A')\n",
    "            chapter = doc.metadata.get('chapter_file', 'N/A')\n",
    "            heading = doc.metadata.get('heading_context', 'N/A')\n",
    "            el_type = doc.metadata.get('element_type', 'N/A')\n",
    "            print(f\"      ‚ñ∂Ô∏è  Metadata: [Book: {book}] [Chapter File: {chapter}] [Heading: {heading}] [Type: {el_type}]\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if not os.environ.get(\"OPENAI_API_KEY\") or os.environ.get(\"OPENAI_API_KEY\") == \"YOUR_API_KEY\":\n",
    "        print(\"üõë WARNING: Please set your OPENAI_API_KEY environment variable.\")\n",
    "    \n",
    "    embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üöÄ STARTING PDF BOOK WORKFLOW with UNSTRUCTURED\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # This path should point to your directory of PDFs\n",
    "    book_source_path = r\"/home/manoj/Project/RAG/kehs1dd\"\n",
    "    \n",
    "    # Dummy directory setup for first-time run\n",
    "    if not os.path.exists(book_source_path):\n",
    "        print(f\"‚ÑπÔ∏è Test directory not found. Creating '{book_source_path}' for demonstration.\")\n",
    "        os.makedirs(book_source_path)\n",
    "        # You MUST replace these with real PDFs for unstructured to work.\n",
    "        print(\"üõë Note: The created directory is empty. You MUST add real PDF files to it for processing.\")\n",
    "\n",
    "    book_faiss_path = \"faiss_index_from_unstructured\"\n",
    "    \n",
    "    # This now calls the unstructured-powered function\n",
    "    book_docs = get_documents_from_source(book_source_path)\n",
    "    \n",
    "    if book_docs:\n",
    "        faiss_store_from_book = create_or_load_faiss_store(book_docs, embedding_model, book_faiss_path)\n",
    "        perform_similarity_search(faiss_store_from_book, \"What is the main theme?\")\n",
    "        perform_similarity_search(faiss_store_from_book, \"Find a character description.\")\n",
    "    else:\n",
    "        print(\"\\nSkipping vector store creation because no documents were extracted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8a506502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8643cca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PROVIDER = \"openai\"\n",
    "MODEL_NAME = \"gpt-4o-mini\"\n",
    "APP_VERSION = 1.0\n",
    "RAG_SYSTEM_PROMPT = \"\"\"you are a tutor, helping students with their problems.\n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation. \n",
    "If you don't know the answer, just say that you don't know.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "06a626ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_existing_faiss_store(index_path, embeddings):\n",
    "    \"\"\"\n",
    "    Loads an existing FAISS vector store from a local path.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(index_path):\n",
    "        print(f\"üõë Error: Index path not found at '{index_path}'.\")\n",
    "        print(\"Please run the main script first to create the vector store.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"‚úÖ Loading existing FAISS index from: {index_path}\")\n",
    "    # The key command to load a FAISS index.\n",
    "    # allow_dangerous_deserialization is required for loading the .pkl file.\n",
    "    vector_store = FAISS.load_local(\n",
    "        index_path, \n",
    "        embeddings,\n",
    "        allow_dangerous_deserialization=True\n",
    "    )\n",
    "    print(\"‚úÖ Index loaded successfully.\")\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "212b83de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loading existing FAISS index from: D:\\functions\\VidyaSetu\\VidyaSetu\\faiss_index_from_unstructured\n",
      "‚úÖ Index loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "faiss_index_path = r\"D:\\functions\\VidyaSetu\\VidyaSetu\\faiss_index_from_unstructured\"\n",
    "faiss_store_from_book = load_existing_faiss_store(faiss_index_path, embeddings=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7b62db5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import nest_asyncio\n",
    "openai_client = OpenAI()\n",
    "nest_asyncio.apply()\n",
    "retriever = faiss_store_from_book.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2119d9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents(question: str):\n",
    "    return retriever.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c8466f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"developer\",\n",
    "            \"content\": RAG_SYSTEM_PROMPT\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"\n",
    "        }\n",
    "    ]\n",
    "    return call_openai(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7260b640",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "def call_openai(\n",
    "    messages: List[dict], model: str = MODEL_NAME, temperature: float = 0.0\n",
    ") -> str:\n",
    "    return openai_client.responses.create(\n",
    "        model=model,\n",
    "        input=messages,\n",
    "        temperature=temperature,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e0739847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(question: str):\n",
    "    documents = retrieve_documents(question)\n",
    "    response = generate_response(question, documents)\n",
    "    return response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "da12a8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "City life began in early societies, particularly in regions where agriculture developed, allowing for the establishment of permanent settlements. This often occurred in fertile areas such as river valleys, where communities could grow crops and support larger populations. Key examples include the Mesopotamian cities between the Tigris and Euphrates rivers, the Nile Valley in Egypt, and the Indus Valley in South Asia. These early cities laid the foundation for the complex societies and empires that followed.\n"
     ]
    }
   ],
   "source": [
    "question = \"Where City life began?\"\n",
    "ai_answer = rag(question)\n",
    "print(ai_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5525d360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mesopotamia means \"land between the rivers\" in Greek. It refers to the region located between the Tigris and Euphrates rivers, which is often considered the cradle of civilization due to its early development of urban society, writing, and complex political structures.\n"
     ]
    }
   ],
   "source": [
    "question = \"Mesopotamia meaning?\"\n",
    "ai_answer = rag(question)\n",
    "print(ai_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vidyasetu-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
